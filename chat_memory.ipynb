{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read the .env file in local directory\n",
    "openai.api_key = os.environ[\n",
    "    \"OPENAI_API_KEY\"\n",
    "]  # to use this notebook we must set up an API key from OpenAI\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo-0125\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # used to interact openai chat models\n",
    "from langchain.chains import ConversationChain # this allows us to create conversational chat pipeline\n",
    "from langchain.memory import ConversationBufferMemory # stores previous conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI() # we called it llm because we are going to use inside of conversation chain memory\n",
    "memory = ConversationBufferMemory() # created buffer memory instance\n",
    "conversation = ConversationChain( \n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True # verbose=False means when we chat with the model model directly gives the answer we don't see internal process but if we change it =True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi this is NoOne\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello NoOne! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi this is NoOne\") # sending message to chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi this is NoOne\n",
      "AI: Hello NoOne! How can I assist you today?\n",
      "Human: what is 8+7?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8 + 7 equals 15.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is 8+7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi this is NoOne\n",
      "AI: Hello NoOne! How can I assist you today?\n",
      "Human: what is 8+7?\n",
      "AI: 8 + 7 equals 15.\n",
      "Human: say my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is NoOne.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"say my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi this is NoOne\n",
      "AI: Hello NoOne! How can I assist you today?\n",
      "Human: what is 8+7?\n",
      "AI: 8 + 7 equals 15.\n",
      "Human: say my name\n",
      "AI: Your name is NoOne.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) # we can see previous conversations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi this is NoOne\\nAI: Hello NoOne! How can I assist you today?\\nHuman: what is 8+7?\\nAI: 8 + 7 equals 15.\\nHuman: say my name\\nAI: Your name is NoOne.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # laod_memory+variables expects input dictionary but it doesn't require any specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi, how are you\"}, {\"output\": \"Thanks for asking I'm fine, and you?\"}) # entering a conversation by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi this is NoOne\n",
      "AI: Hello NoOne! How can I assist you today?\n",
      "Human: what is 8+7?\n",
      "AI: 8 + 7 equals 15.\n",
      "Human: say my name\n",
      "AI: Your name is NoOne.\n",
      "Human: Hi, how are you\n",
      "AI: Thanks for asking I'm fine, and you?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi this is NoOne\\nAI: Hello NoOne! How can I assist you today?\\nHuman: what is 8+7?\\nAI: 8 + 7 equals 15.\\nHuman: say my name\\nAI: Your name is NoOne.\\nHuman: Hi, how are you\\nAI: Thanks for asking I'm fine, and you?\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When creating chatbots or memory needed services bot have to remember the conversation that's done. \n",
    "# But meanwhile remembering the whole conversation would cost much (next token prediction happens based on all previous tokens)\n",
    "# So overcome this issue langchain provides window buffer memory that will store windows of memory\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1) # k=1 is our window length that means it'll remember just previous message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# as we can see it doesn't remember first message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI() # we called it llm because we are going to use inside of conversation chain memory\n",
    "memory = ConversationBufferWindowMemory(k=1) # created buffer memory instance\n",
    "conversation = ConversationChain( \n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=False # verbose=False means when we chat with the model model directly gives the answer we don't see internal process but if we change it =True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, NoOne! How can I assist you today?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi this is NoOne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The United States sent the Apollo 11 mission to the Moon on July 16, 1969. The mission successfully landed on the Moon on July 20, 1969, with astronaut Neil Armstrong becoming the first person to walk on the lunar surface.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"When did US send rocket to Moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Say my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationTokenBufferMemory is same but limit is determined by token count\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50) # when limit is reached it takes into account last conversations\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # tiktoken is tokenization library by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tiktoken.get_encoding(\"cl100k_base\") # this is the tokenizer for gpt3.5-turbo also the line below works\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0125\") # give model name and it chooses right encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_conv = encoding.encode(memory.load_memory_variables({})[\"history\"]) # encoding conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Human',\n",
       " b':',\n",
       " b' AI',\n",
       " b' is',\n",
       " b' what',\n",
       " b'?',\n",
       " b'!\\n',\n",
       " b'AI',\n",
       " b':',\n",
       " b' Amazing',\n",
       " b'!\\n',\n",
       " b'Human',\n",
       " b':',\n",
       " b' Back',\n",
       " b'prop',\n",
       " b'agation',\n",
       " b' is',\n",
       " b' what',\n",
       " b'?\\n',\n",
       " b'AI',\n",
       " b':',\n",
       " b' Beautiful',\n",
       " b'!\\n',\n",
       " b'Human',\n",
       " b':',\n",
       " b' Chat',\n",
       " b'bots',\n",
       " b' are',\n",
       " b' what',\n",
       " b'?\\n',\n",
       " b'AI',\n",
       " b':',\n",
       " b' Ch',\n",
       " b'arming',\n",
       " b'!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in encoded_conv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conversation summary buffer memory LLM creates a summary and continues based on that\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"Your day kicks off at 7:30 AM with a strategy meeting with the marketing team—bring your notes on the new campaign.\\\n",
    "At 10 AM, you have a deep dive into model optimization, so set up your environment beforehand.\\\n",
    "From 1 PM to 3 PM, you'll be reviewing code for the fraud detection project, making sure everything is on track.\\\n",
    "At 4 PM, there's a brainstorming session on future AI integrations, followed by a quick catch-up with your mentor at 6 PM.\\\n",
    "Wrap up the day with some quiet time to reflect on new ideas before heading home.\"\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the day's schedule. The AI outlines a busy day filled with meetings, code reviews, and brainstorming sessions.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the day's schedule. The AI outlines a busy day filled with meetings, code reviews, and brainstorming sessions.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A good demo to show could be a new feature or functionality that has been recently developed and is ready for testing. It could showcase the user interface design, the seamless integration with existing systems, or the improved performance metrics. Another option could be a prototype of a future project to gather feedback and generate excitement among stakeholders. Ultimately, the demo should highlight the team's progress and innovation in a visually engaging and informative way.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
